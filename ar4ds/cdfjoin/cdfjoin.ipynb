{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import minhash\n",
    "from loadimdbdata import table_pairs\n",
    "import random\n",
    "import time\n",
    "from math import factorial\n",
    "from datetime import datetime\n",
    "from scipy.special import comb\n",
    "from scipy import linspace\n",
    "from statistics import median\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATING CDF AND VALUE THRESHOLD GIVEN CDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating cdf\n",
    "\n",
    "def unit_step_function(a, t):\n",
    "    if t < a:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a number of random variables with uniform distribution, \n",
    "# find probability that their sum = given target value\n",
    "\n",
    "def cdf(n, t):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        mySum = 0.0\n",
    "        for i in range(n+1):\n",
    "            mySum += ((-1)**i * comb(n, i) * ((t-i)**n)/factorial(n) * unit_step_function(i, t))\n",
    "        return mySum\n",
    "    \n",
    "def cdf_multiple_values(n, t):\n",
    "    cdf_values = []\n",
    "    for target_value in t:\n",
    "        cdf_values.append(cdf(n, target_value))\n",
    "    return cdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a number of random variables with uniform distribution,\n",
    "# find the threshold for their sum at a certain probability\n",
    "\n",
    "def find_value_threshold(n, cdf_value):\n",
    "    cdf_value *= 1.0\n",
    "    t_index = 0\n",
    "    while cdf(n, t_index) < cdf_value:\n",
    "        t_index+=1\n",
    "    if cdf_value -.001 <= cdf(n, t_index) <= cdf_value + .001:\n",
    "        return t_index\n",
    "    else:\n",
    "        lower_bound = t_index-1.0\n",
    "        bound_difference = 1.0\n",
    "        midpoint = lower_bound + bound_difference/2.0\n",
    "        bound_difference/=2.0\n",
    "        midpoint_cdf = cdf(n, midpoint)\n",
    "        while not cdf_value - .01 <= midpoint_cdf <= cdf_value+.01:\n",
    "            if midpoint_cdf > cdf_value:\n",
    "                midpoint -= bound_difference/2.0\n",
    "            else:\n",
    "                midpoint += bound_difference/2.0\n",
    "            midpoint_cdf = cdf(n, midpoint)\n",
    "            bound_difference /= 2.0\n",
    "        return midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_plotter(n, multiple_n = False):\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Sum')\n",
    "    if multiple_n:\n",
    "        for number in n:\n",
    "            cdf_plotter(number)\n",
    "    else:\n",
    "        x = linspace(0, n+2, 500)\n",
    "        plt.plot(x, cdf_multiple_values(n,x), label='n = {i}'.format(i=n))\n",
    "        plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_value_plotter(n, cdf, multiple_n = False, multiple_cdf = False):\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Sum')\n",
    "    if multiple_n:\n",
    "        for n_value in n:\n",
    "            x = linspace(0, n_value+2, 500)\n",
    "            plt.plot(x, cdf_multiple_values(n_value,x), label='n = {i}'.format(i=n_value))\n",
    "            plt.legend(loc='best')\n",
    "            if multiple_cdf:\n",
    "                for cdf_value in cdf:\n",
    "                    x = find_value_threshold(n_value, cdf_value)\n",
    "                    y = cdf_value\n",
    "                    plt.plot(x, y, marker='o', color='black', label='t = {i}'.format(i=x))\n",
    "            else:\n",
    "                x = find_value_threshold(n_value, cdf)\n",
    "                y = cdf\n",
    "                plt.plot(x, y, market='o', color='black', label='t = {i}'.format(i=x))\n",
    "    else:\n",
    "        x = linspace(0, n+2, 500)\n",
    "        plt.plot(x, cdf_multiple_values(n,x), label='n = {i}'.format(i=n))\n",
    "        plt.legend(loc='best')\n",
    "        if multiple_cdf:\n",
    "            for cdf_value in cdf:\n",
    "                x = find_value_threshold(n, cdf_value)\n",
    "                y = cdf_value\n",
    "                plt.plot(x, y, marker='o', color='black', label='t = {i}'.format(i=x))\n",
    "        else:\n",
    "            x = find_value_threshold(n, cdf)\n",
    "            y = cdf\n",
    "            plt.plot(x, y, marker='o', color='black', label='t = {i}'.format(i=x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERFORMING A NATURAL JOIN USING A CLASSIC HASH JOIN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for edge case where tables have no common attributes\n",
    "\n",
    "def cartesian_product(r,s):\n",
    "    output = []\n",
    "    for r_entry in r:\n",
    "        for s_entry in s:\n",
    "            r_copy = r_entry.copy()\n",
    "            r_copy.update(s_entry)\n",
    "            output.append(r_copy)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash join function for two tables\n",
    "\n",
    "def classic_hash_join(r, s):\n",
    "    if len(s) == 0 or len(r) == 0:\n",
    "        return []\n",
    "    \n",
    "    r_attributes = r[0].keys()\n",
    "    s_attributes = s[0].keys()\n",
    "    common_attributes = list(filter(lambda ra: ra in s_attributes, r_attributes))\n",
    "    if len(common_attributes) > 0:\n",
    "        s_unique_attributes = list(filter(lambda sa: sa not in common_attributes, s_attributes))\n",
    "        hashtable = {}\n",
    "        hashtable_keys = []\n",
    "        hashkey = \"\"\n",
    "        \n",
    "        #build hash table\n",
    "        row_index = 0\n",
    "        for entry in r:\n",
    "            hashkey = \"\".join([entry[attribute] for attribute in common_attributes if attribute in entry.keys()])\n",
    "            if hashkey not in hashtable_keys:\n",
    "                hashtable[hashkey] = [row_index]\n",
    "                hashtable_keys.append(hashkey)\n",
    "            else:\n",
    "                hashtable[hashkey].append(row_index)\n",
    "            hashkey = \"\"\n",
    "            row_index+=1\n",
    "        \n",
    "        #join\n",
    "        output = []\n",
    "        for entry in s:\n",
    "            hashkey = \"\".join([entry[attribute] for attribute in common_attributes if attribute in entry.keys()])\n",
    "            if hashkey in hashtable_keys:\n",
    "                filtered_dict = {k:v for (k,v) in entry.items() if k in s_unique_attributes}\n",
    "                for r_index in hashtable[hashkey]:\n",
    "                    new_entry = (r[r_index].copy())\n",
    "                    new_entry.update(filtered_dict)\n",
    "                    output.append(new_entry)\n",
    "            hashkey=\"\"\n",
    "            \n",
    "        return output\n",
    "    else:\n",
    "        return cartesian_product(r,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function for performing hash joins on given list of tables\n",
    "\n",
    "def natural_join(tables):\n",
    "    l1 = tables[0]\n",
    "    for i in range(1, len(tables)):\n",
    "        l1 = classic_hash_join(l1, tables[i])\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FETCHING DATA FOR PERFORMING JOINS WITH HASHED AND RANDOM SAMPLING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for normalizing hash values to [0,1]\n",
    "\n",
    "def normalize(lower, upper, value):\n",
    "    return (float(value) - float(lower)) / (float(upper) - float(lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for cleaning tables after hashing join attribute values\n",
    "\n",
    "def remove_hashsum_key(tables):\n",
    "    for table in tables:\n",
    "        for entry in table:\n",
    "            if \"hash sum\" in entry.keys():\n",
    "                del entry[\"hash sum\"]\n",
    "                \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data for performing joins with hashed and random sampling,\n",
    "# given a list of tables\n",
    "\n",
    "def cdfjoin(tables, sampling_threshold, random_threshold=0.0):\n",
    "    if len(tables) <= 1:\n",
    "        return tables\n",
    "    else:\n",
    "        attrs = {}\n",
    "        join_attrs = []\n",
    "        \n",
    "        #find join attributes\n",
    "        for i in range(len(tables)):\n",
    "            table = tables[i]\n",
    "            for table_key in table[0].keys():\n",
    "                if table_key in attrs.keys(): attrs[table_key].append(i)\n",
    "                else: attrs[table_key] = [i]\n",
    "        join_attrs = [k for k,v in attrs.items() if len(v) > 1]\n",
    "        \n",
    "        #generate hash functions for each join_attr\n",
    "        hash_functions = minhash._generate_hash_fns(len(join_attrs))\n",
    "        attrs_hash_dict = {join_attrs[i]: hash_functions[i] for i in range(len(hash_functions))}\n",
    "        \n",
    "        #copy tables\n",
    "        tables_copy = []\n",
    "        for table in tables:\n",
    "            tables_copy.append(deepcopy(table))\n",
    "        \n",
    "        #calculate hash sum for each entry in each table\n",
    "        for table in tables_copy:\n",
    "            table_attrs = [key for key in table[0].keys() if key in join_attrs]\n",
    "            norm_hashed_table = {}\n",
    "            hashed_table = {}\n",
    "            hashed_table = {attr: minhash._min_hash([attr], table, attrs_hash_dict[attr]) for attr in table_attrs}\n",
    "            for k,v in hashed_table.items():\n",
    "                norm_v = {normalize(0, minhash.NEXTPRIME-1, v1): v2 for v1, v2 in v.items()}\n",
    "                norm_hashed_table[k] = norm_v\n",
    "            for i in range(len(table)):\n",
    "                entry = table[i]\n",
    "                hash_scores = []\n",
    "                for k, v in norm_hashed_table.items():\n",
    "                    for v1, v2 in v.items():\n",
    "                        if i in v2:\n",
    "                            hash_scores.append(v1)\n",
    "                            break\n",
    "                entry[\"hash sum\"] = sum(hash_scores)\n",
    "        \n",
    "        #filter for all entries whose cdf <= sampling probability\n",
    "        filtered_tables = []\n",
    "        start = time.time()\n",
    "        for i in range(len(tables)):\n",
    "            table = tables_copy[i]\n",
    "            filtered_table = []\n",
    "            if \"hash sum\" in table[0].keys():\n",
    "                n_join_attrs = len([i for i in table[0].keys() if i in join_attrs])\n",
    "                for entry in table:\n",
    "                    if cdf(n_join_attrs, entry[\"hash sum\"]) <= sampling_threshold:\n",
    "                        filtered_table.append(entry)\n",
    "            else:\n",
    "                filtered_table = table\n",
    "            if len(filtered_table) > 0:\n",
    "                filtered_tables.append(filtered_table)\n",
    "            else:\n",
    "                filtered_tables = []\n",
    "                break\n",
    "        filtered_time = time.time() - start        \n",
    "\n",
    "                \n",
    "        #filter for all entries whose cdf > random probability\n",
    "        random_tables = []\n",
    "        start = time.time()\n",
    "        for i in range(len(tables)):\n",
    "            table = tables_copy[i]\n",
    "            if random_threshold == 0.0:\n",
    "                random_threshold = random.uniform(0,1)\n",
    "            random_table = []\n",
    "            if \"hash sum\" in table[0].keys():\n",
    "                n_join_attrs = len([i for i in table[0].keys() if i in join_attrs])\n",
    "                for entry in table:\n",
    "                    if random_threshold < cdf(n_join_attrs, entry[\"hash sum\"]):\n",
    "                        random_table.append(entry)\n",
    "            else:\n",
    "                random_table = table\n",
    "            if len(random_table) > 0:\n",
    "                random_tables.append(random_table)\n",
    "            else:\n",
    "                random_tables = []\n",
    "                break\n",
    "        random_time = time.time() - start\n",
    "        \n",
    "        if filtered_tables == []:\n",
    "            joined_filtered_tables = []\n",
    "        else:\n",
    "            filtered_tables = remove_hashsum_key(filtered_tables)\n",
    "            start = time.time()\n",
    "            joined_filtered_tables = natural_join(filtered_tables)\n",
    "            filtered_time += time.time() - start\n",
    "            \n",
    "        if random_tables == []:\n",
    "            joined_random_tables = []\n",
    "        else:\n",
    "            random_tables = remove_hashsum_key(random_tables)\n",
    "            start = time.time()\n",
    "            joined_random_tables = natural_join(random_tables)\n",
    "            random_time += time.time() - start\n",
    "            \n",
    "        return [(joined_filtered_tables, filtered_time), (joined_random_tables, random_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING DATA ON JOINED TABLE SIZE AND RUNTIME FOR HASHED AND RANDOM SAMPLING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for fetching data on multiple values\n",
    "\n",
    "def get_cdfjoin_plot_data(tables, x_values, size = True, runtime = True):\n",
    "    cdfjoin_data = []\n",
    "    for threshold in x_values:\n",
    "        cdfjoin_result = cdfjoin(tables, threshold)\n",
    "        if size and runtime:\n",
    "            cdfjoin_data.append([(len(cdfjoin_result[0][0]), cdfjoin_result[0][1]),\n",
    "                                (len(cdfjoin_result[1][0]), cdfjoin_result[1][1])])\n",
    "        elif size and not runtime:\n",
    "            cdfjoin_data.append([(len(cdfjoin_result[0][0]), None), (len(cdfjoin_result[1][0]), None)])\n",
    "        elif not size and runtime:\n",
    "            cdfjoin_data.append([(None, cdfjoin_result[0][1]), (None, cdfjoin_result[1][1])])\n",
    "            \n",
    "    return cdfjoin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cdfjoin_plots(tables, size = True, runtime = True):\n",
    "    if not size and not runtime:\n",
    "        print(\"Error: Select at least one of the following types of data to display: size, runtime\")\n",
    "        \n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25,8))\n",
    "        fig = plt.subplots_adjust(wspace=.5)\n",
    "        x = linspace(0, 1, 100)\n",
    "        cdfjoin_plot_data = get_cdfjoin_plot_data(tables, x, size, runtime)\n",
    "        \n",
    "        if size:\n",
    "            outputsize_hashed = [i[0] for [(i),(j)] in cdfjoin_plot_data]\n",
    "            outputsize_random = [j[0] for [(i),(j)] in cdfjoin_plot_data]\n",
    "            ax1.plot(x, outputsize_hashed, label='Hashed sampling')\n",
    "            ax1.plot(x, outputsize_random, label='Random sampling')\n",
    "            ax1.legend(loc='best')\n",
    "            ax1.set_xlabel('Sampling Threshold')\n",
    "            ax1.set_ylabel('Output size')\n",
    "            ax1.set_title('Output size vs. Sampling Threshold')\n",
    "        \n",
    "        if runtime:\n",
    "            runtime_hashed = [i[1] for [(i),(j)] in cdfjoin_plot_data]\n",
    "            runtime_random = [j[1] for [(i),(j)] in cdfjoin_plot_data]\n",
    "            ax2.plot(x, runtime_hashed, label='Hashed sampling')\n",
    "            ax2.plot(x, runtime_random, label='Random sampling')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.set_xlabel('Sampling Threshold')\n",
    "            ax2.set_ylabel('Runtime')\n",
    "            ax2.set_title('Runtime vs. Sampling Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cdfjoin_plots_multiple(num_trials, tables, size = True, runtime = True):\n",
    "    if not size and not runtime:\n",
    "        print(\"Error: Select at least one of the following types of data to display: size, runtime\")\n",
    "        \n",
    "    else:\n",
    "        x = linspace(0, 1, 100)\n",
    "        size_hashed_data_1000 = {}\n",
    "        size_random_data_1000 = {}\n",
    "        runtime_hashed_data_1000 = {}\n",
    "        runtime_random_data_1000 = {}\n",
    "        \n",
    "#         gathering data for 1000 trials\n",
    "#         num_trials = 1000\n",
    "        for i in range(num_trials): \n",
    "            trial_data = get_cdfjoin_plot_data(tables, x, size, runtime)\n",
    "            for j in range(len(x)):\n",
    "                x_value = x[j]\n",
    "                trial_values = trial_data[j]\n",
    "                if size:\n",
    "                    if x_value in size_hashed_data_1000.keys():\n",
    "                        size_hashed_data_1000[x_value].append(trial_values[0][0])\n",
    "                        size_random_data_1000[x_value].append(trial_values[1][0])\n",
    "                    else:\n",
    "                        size_hashed_data_1000[x_value] = [trial_values[0][0]]\n",
    "                        size_random_data_1000[x_value] = [trial_values[1][0]]\n",
    "\n",
    "                if runtime:\n",
    "                    if x_value in runtime_hashed_data_1000.keys():\n",
    "                        runtime_hashed_data_1000[x_value].append(trial_values[0][1])\n",
    "                        runtime_random_data_1000[x_value].append(trial_values[1][1])\n",
    "                    else:\n",
    "                        runtime_hashed_data_1000[x_value] = [trial_values[0][1]]\n",
    "                        runtime_random_data_1000[x_value] = [trial_values[1][1]]\n",
    "                    \n",
    "            \n",
    "#         organizing data for each trial by x-value (sampling threshold)\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25,8))\n",
    "        fig = plt.subplots_adjust(wspace=.5)\n",
    "        \n",
    "        if size:\n",
    "            size_hashed_data = []\n",
    "            size_random_data = []\n",
    "            for k,v in size_hashed_data_1000.items():\n",
    "                size_hashed_data.append(median(v))\n",
    "            for k,v in size_random_data_1000.items():\n",
    "                size_random_data.append(median(v))\n",
    "            ax1.plot(x, size_hashed_data, label='Hashed sampling')\n",
    "            ax1.plot(x, size_random_data, label='Random sampling')\n",
    "            ax1.legend(loc='best')\n",
    "            ax1.set_xlabel('Sampling Threshold')\n",
    "            ax1.set_ylabel('Output size')\n",
    "            ax1.set_title('Output size vs. Sampling Threshold')\n",
    "                \n",
    "                \n",
    "        if runtime:\n",
    "            runtime_hashed_data = []\n",
    "            runtime_random_data = []\n",
    "            for k,v in runtime_hashed_data_1000.items():\n",
    "                runtime_hashed_data.append(median(v))\n",
    "            for k,v in runtime_random_data_1000.items():\n",
    "                runtime_random_data.append(median(v))\n",
    "            ax2.plot(x, runtime_hashed_data, label='Hashed sampling')\n",
    "            ax2.plot(x, runtime_random_data, label='Random sampling')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.set_xlabel('Sampling Threshold')\n",
    "            ax2.set_ylabel('Runtime')\n",
    "            ax2.set_title('Runtime vs. Sampling Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cdfjoin_plots_multiple(100, table_pairs[0], True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
